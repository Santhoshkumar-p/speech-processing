{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UR4qfYrVoO4v"
      },
      "source": [
        "# Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mA9qZoIDcx-h"
      },
      "outputs": [],
      "source": [
        "%pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchtext==0.14.1 torchaudio==0.13.1 torchdata==0.5.1 --extra-index-url https://download.pytorch.org/whl/cu117 -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONgAWhqdoYy-"
      },
      "source": [
        "\n",
        "This may take a while"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SS7a7xeEoaV9",
        "outputId": "7f34d495-256d-46de-d881-79e5d978c628"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'ctcdecode' already exists and is not an empty directory.\n",
            "/content/ctcdecode\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ctcdecode (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb --quiet\n",
        "!pip install python-Levenshtein -q\n",
        "!git clone --recursive https://github.com/parlance/ctcdecode.git\n",
        "!pip install wget -q\n",
        "%cd ctcdecode\n",
        "!pip install . -q\n",
        "%cd ..\n",
        "\n",
        "!pip install torchsummaryX -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKLIou7YPo2Q",
        "outputId": "b0814f1f-019b-436c-d2d3-11e9b17e21be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchsummaryX==1.3.0 in /usr/local/lib/python3.10/dist-packages (1.3.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchsummaryX==1.3.0) (1.13.1+cu117)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchsummaryX==1.3.0) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from torchsummaryX==1.3.0) (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->torchsummaryX==1.3.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->torchsummaryX==1.3.0) (2023.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchsummaryX==1.3.0) (4.9.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->torchsummaryX==1.3.0) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "If torchsummaryX doesn't work, please run this cell. Alternatively, please refer to Piazza post @209 for more assistance:\n",
        "'''\n",
        "\n",
        "!pip install torchsummaryX==1.3.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWVONJxCobPc"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78ZTCIXoof2f",
        "outputId": "279f7e27-0b8d-45c1-f986-a29b1901ec6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummaryX import summary\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "from torch.nn.utils.rnn import PackedSequence\n",
        "import torchaudio.transforms as tat\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "import gc\n",
        "\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "# imports for decoding and distance calculation\n",
        "import ctcdecode\n",
        "import Levenshtein\n",
        "from ctcdecode import CTCBeamDecoder\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gg3-yJ8tok34"
      },
      "source": [
        "# Kaggle Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdUelfGhom1m",
        "outputId": "4a0d8760-47da-43fc-8aa5-963e8c868fa1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/59.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8 -q\n",
        "!mkdir /root/.kaggle\n",
        "\n",
        "with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
        "    f.write('{\"username\":\"\",\"key\":\"\"}') # TODO: Put your kaggle username & key here\n",
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSjBwfXeoq4B",
        "outputId": "5db41e2a-d887-425c-9c25-bf7834686c3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading hw3p2asr-s24.zip to /content\n",
            "100% 3.73G/3.74G [00:36<00:00, 106MB/s]\n",
            "100% 3.74G/3.74G [00:36<00:00, 110MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle competitions download -c hw3p2asr-s24"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ruxWP60LCQA",
        "outputId": "61a373af-6fc9-47b1-9960-e50058528f59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11-785-s24-hw3p2  ctcdecode  hw3p2asr-s24.zip  sample_data\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "This will take a couple minutes, but you should see at least the following:\n",
        "11-785-s24-hw3p2  ctcdecode  hw3p2asr-s24.zip  sample_data\n",
        "'''\n",
        "!unzip -q hw3p2asr-s24.zip\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9v5ewZDMpYA"
      },
      "source": [
        "# Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Cp-716IMZRd"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ORNHnSFroP0"
      },
      "source": [
        "# Dataset and Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0v7wHRWrqH6"
      },
      "outputs": [],
      "source": [
        "# ARPABET PHONEME MAPPING\n",
        "# DO NOT CHANGE\n",
        "\n",
        "CMUdict_ARPAbet = {\n",
        "    \"\" : \" \",\n",
        "    \"[SIL]\": \"-\", \"NG\": \"G\", \"F\" : \"f\", \"M\" : \"m\", \"AE\": \"@\",\n",
        "    \"R\"    : \"r\", \"UW\": \"u\", \"N\" : \"n\", \"IY\": \"i\", \"AW\": \"W\",\n",
        "    \"V\"    : \"v\", \"UH\": \"U\", \"OW\": \"o\", \"AA\": \"a\", \"ER\": \"R\",\n",
        "    \"HH\"   : \"h\", \"Z\" : \"z\", \"K\" : \"k\", \"CH\": \"C\", \"W\" : \"w\",\n",
        "    \"EY\"   : \"e\", \"ZH\": \"Z\", \"T\" : \"t\", \"EH\": \"E\", \"Y\" : \"y\",\n",
        "    \"AH\"   : \"A\", \"B\" : \"b\", \"P\" : \"p\", \"TH\": \"T\", \"DH\": \"D\",\n",
        "    \"AO\"   : \"c\", \"G\" : \"g\", \"L\" : \"l\", \"JH\": \"j\", \"OY\": \"O\",\n",
        "    \"SH\"   : \"S\", \"D\" : \"d\", \"AY\": \"Y\", \"S\" : \"s\", \"IH\": \"I\",\n",
        "    \"[SOS]\": \"[SOS]\", \"[EOS]\": \"[EOS]\"\n",
        "}\n",
        "\n",
        "CMUdict = list(CMUdict_ARPAbet.keys())\n",
        "ARPAbet = list(CMUdict_ARPAbet.values())\n",
        "\n",
        "\n",
        "PHONEMES = CMUdict[:-2]\n",
        "LABELS = ARPAbet[:-2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eN2kcxwXLLBb",
        "outputId": "16242b3a-004e-4e11-b0b7-b1249b04b90c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2703\n"
          ]
        }
      ],
      "source": [
        "# You might want to play around with the mapping as a sanity check here\n",
        "import os\n",
        "files = os.listdir('../content/11-785-s24-hw3p2/dev-clean/mfcc')\n",
        "print(f'{len(files)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "up7NXa2Y7EdS",
        "outputId": "7e04c954-e154-467d-ba4b-b0adb48283b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(135,)\n",
            "(915, 27)\n",
            "[12]\n",
            "Length of labels 119 and ['[SOS]' '[SIL]' 'AE' 'T' 'AH' 'N' 'AH' 'DH' 'ER' 'T' 'AY' 'M' 'W' 'EH'\n",
            " 'N' 'AY' 'W' 'AA' 'Z' 'F' 'AO' 'R' 'T' 'IY' 'N' 'Y' 'IH' 'R' 'Z' 'OW' 'L'\n",
            " 'D' 'W' 'IY' 'HH' 'AE' 'D' 'JH' 'AH' 'S' 'T' 'L' 'EH' 'F' 'T' 'F' 'AO'\n",
            " 'R' 'T' 'EH' 'L' 'IH' 'S' 'AA' 'N' 'DH' 'AH' 'S' 'IH' 'N' 'AH' 'B' 'AW'\n",
            " 'N' 'R' 'IH' 'V' 'ER' '[SIL]' 'AH' 'N' 'D' 'M' 'AY' 'Y' 'AH' 'NG' 'G'\n",
            " 'AH' 'S' 'T' 'AH' 'NG' 'K' 'AH' 'L' 'HH' 'AE' 'D' 'S' 'AH' 'L' 'EH' 'K'\n",
            " 'T' 'AH' 'D' 'AH' 'F' 'AY' 'N' 'S' 'P' 'AA' 'T' 'F' 'AO' 'R' 'AW' 'ER'\n",
            " 'N' 'AY' 'T' 'K' 'AE' 'M' 'P' '[SIL]' '[EOS]']\n"
          ]
        }
      ],
      "source": [
        "mfcc = '../content/11-785-s24-hw3p2/dev-clean/mfcc/' + files[1]\n",
        "tran = '../content/11-785-s24-hw3p2/dev-clean/transcript/' + files[1]\n",
        "vect = np.load(mfcc)\n",
        "dat_pad = np.pad(vect, ((5, 5), (0, 0)), mode='constant', constant_values=0)\n",
        "print(dat_pad[-5:].flatten().shape)\n",
        "print(vect.shape)\n",
        "trans = np.load(tran)\n",
        "print(np.where(np.array(PHONEMES) == 'UH')[0])\n",
        "print(f'Length of labels {len(trans)} and {trans}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "va8GXfSj7NU4",
        "outputId": "91bd4651-d484-4b68-f596-52a42651884b"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "mfcc_data= np.swapaxes(vect, 0 ,1)\n",
        "cax = plt.imshow(mfcc_data, interpolation='nearest', cmap=cm.Oranges_r, origin='lower')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MDxlHo97QVD",
        "outputId": "38808415-1aa3-40da-f856-4459eb7c11fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(915, 27)\n",
            "(915, 27)\n",
            "(24705,)\n"
          ]
        }
      ],
      "source": [
        "mfcc = vect\n",
        "print(vect.shape)\n",
        "mfcc = ((mfcc) - (np.mean(mfcc, axis=0))) / (np.sqrt(np.var(mfcc, axis=0)))\n",
        "print(mfcc.shape)\n",
        "concat = np.concatenate(mfcc, axis=0)\n",
        "print(concat.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agmNBKf4JrLV"
      },
      "source": [
        "### Train Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afd0_vlbJmr_"
      },
      "outputs": [],
      "source": [
        "class AudioDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    # For this homework, we give you full flexibility to design your data set class.\n",
        "    # Hint: The data from HW1 is very similar to this HW\n",
        "\n",
        "    #TODO\n",
        "    def __init__(self, root, phonemes = PHONEMES, partition= \"train-clean-100\"):\n",
        "        '''\n",
        "        Initializes the dataset.\n",
        "\n",
        "        INPUTS: What inputs do you need here?\n",
        "        '''\n",
        "\n",
        "        # Load the directory and all files in them\n",
        "\n",
        "        self.mfcc_dir = os.path.join(root, partition, 'mfcc')\n",
        "        self.transcript_dir = os.path.join(root, partition, 'transcript')\n",
        "\n",
        "        self.mfcc_files = sorted(os.listdir(self.mfcc_dir))\n",
        "        self.transcript_files = sorted(os.listdir(self.transcript_dir))\n",
        "\n",
        "        self.PHONEMES = PHONEMES\n",
        "\n",
        "        #TODO\n",
        "        # WHAT SHOULD THE LENGTH OF THE DATASET BE?\n",
        "        assert len(self.mfcc_files) == len(self.transcript_files)\n",
        "        self.length = len(self.mfcc_files)\n",
        "\n",
        "        #TODO\n",
        "        # HOW CAN WE REPRESENT PHONEMES? CAN WE CREATE A MAPPING FOR THEM?\n",
        "        # HINT: TENSORS CANNOT STORE NON-NUMERICAL VALUES OR STRINGS\n",
        "        self.label_to_index = {label: index for index, label in enumerate(LABELS)}\n",
        "        self.phoneme_to_index = {phoneme: index for index, phoneme in enumerate(PHONEMES)}\n",
        "\n",
        "        #TODO\n",
        "        # CREATE AN ARRAY OF ALL FEATUERS AND LABELS\n",
        "        # WHAT NORMALIZATION TECHNIQUE DID YOU USE IN HW1? CAN WE USE IT HERE?\n",
        "        '''\n",
        "        You may decide to do this in __getitem__ if you wish.\n",
        "        However, doing this here will make the __init__ function take the load of\n",
        "        loading the data, and shift it away from training.\n",
        "        '''\n",
        "        self.mfccs, self.transcripts = [], []\n",
        "        for i in range(len(self.mfcc_files)):\n",
        "        #   Load a single mfcc\n",
        "            mfcc = np.load(os.path.join(self.mfcc_dir, self.mfcc_files[i]))\n",
        "        #   Do Cepstral Normalization of mfcc (explained in writeup)\n",
        "            mfcc = ((mfcc) - (np.mean(mfcc, axis=0))) / (np.std(mfcc, axis=0) + 1e-5)\n",
        "            transcript  = np.load(os.path.join(self.transcript_dir, self.transcript_files[i]))\n",
        "            trans_indices = np.array([np.where((np.array(self.PHONEMES) == trans))[0] for trans in transcript[1:-1]])\n",
        "            self.mfccs.append(mfcc)\n",
        "            self.transcripts.append(trans_indices)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        '''\n",
        "        TODO: What do we return here?\n",
        "        '''\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        '''\n",
        "        TODO: RETURN THE MFCC COEFFICIENTS AND ITS CORRESPONDING LABELS\n",
        "\n",
        "        If you didn't do the loading and processing of the data in __init__,\n",
        "        do that here.\n",
        "\n",
        "        Once done, return a tuple of features and labels.\n",
        "        '''\n",
        "        mfcc = torch.tensor(self.mfccs[ind]) # TODO\n",
        "        transcript = torch.tensor(self.transcripts[ind]) # TODO\n",
        "        return mfcc, transcript\n",
        "\n",
        "\n",
        "    def collate_fn(self,batch):\n",
        "        '''\n",
        "        TODO:\n",
        "        1.  Extract the features and labels from 'batch'\n",
        "        2.  We will additionally need to pad both features and labels,\n",
        "            look at pytorch's docs for pad_sequence\n",
        "        3.  This is a good place to perform transforms, if you so wish.\n",
        "            Performing them on batches will speed the process up a bit.\n",
        "        4.  Return batch of features, labels, lenghts of features,\n",
        "            and lengths of labels.\n",
        "        '''\n",
        "        # batch of input mfcc coefficients\n",
        "        # batch of output phonemes\n",
        "        batch_mfcc = [i[0] for i in batch]\n",
        "        batch_transcript = [i[1] for i in batch] # TODO\n",
        "\n",
        "        # HINT: CHECK OUT -> pad_sequence (imported above)\n",
        "        # Also be sure to check the input format (batch_first)\n",
        "        batch_mfcc_pad = pad_sequence(batch_mfcc, batch_first=True) # TODO\n",
        "        lengths_mfcc = [len(f) for f in batch_mfcc] # TODO\n",
        "\n",
        "        batch_transcript_pad = pad_sequence(batch_transcript, batch_first=True) # TODO\n",
        "        lengths_transcript = [len(l) for l in batch_transcript] # TODO\n",
        "\n",
        "        # You may apply some transformation, Time and Frequency masking, here in the collate function;\n",
        "        # Food for thought -> Why are we applying the transformation here and not in the __getitem__?\n",
        "        #                  -> Would we apply transformation on the validation set as well?\n",
        "        #                  -> Is the order of axes / dimensions as expected for the transform functions?\n",
        "\n",
        "        # Return the following values: padded features, padded labels, actual length of features, actual length of the labels\n",
        "        return batch_mfcc_pad, torch.squeeze(batch_transcript_pad, dim=2), torch.tensor(lengths_mfcc), torch.tensor(lengths_transcript)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqDrxeHfJw4g"
      },
      "source": [
        "### Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrLS1wfVJppA"
      },
      "outputs": [],
      "source": [
        "# Test Dataloader\n",
        "#TODO\n",
        "class AudioDatasetTest(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, phonemes = PHONEMES, partition= \"test-clean\"):\n",
        "\n",
        "          self.mfcc_dir = os.path.join(root, partition, 'mfcc')\n",
        "\n",
        "          self.mfcc_files = sorted(os.listdir(self.mfcc_dir))\n",
        "\n",
        "          self.PHONEMES = PHONEMES\n",
        "\n",
        "          self.length = len(self.mfcc_files)\n",
        "\n",
        "          self.mfccs = []\n",
        "          for i in range(len(self.mfcc_files)):\n",
        "          #   Load a single mfcc\n",
        "              mfcc = np.load(os.path.join(self.mfcc_dir, self.mfcc_files[i]))\n",
        "          #   Do Cepstral Normalization of mfcc (explained in writeup)\n",
        "              mfcc = ((mfcc) - (np.mean(mfcc, axis=0))) / (np.std(mfcc, axis=0) + 1e-5)\n",
        "              self.mfccs.append(mfcc)\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        '''\n",
        "        TODO: What do we return here?\n",
        "        '''\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        mfcc = torch.tensor(self.mfccs[ind])\n",
        "        return mfcc\n",
        "\n",
        "\n",
        "    def collate_fn(self,batch):\n",
        "        '''\n",
        "        TODO:\n",
        "        1.  Extract the features and labels from 'batch'\n",
        "        2.  We will additionally need to pad both features and labels,\n",
        "            look at pytorch's docs for pad_sequence\n",
        "        3.  This is a good place to perform transforms, if you so wish.\n",
        "            Performing them on batches will speed the process up a bit.\n",
        "        4.  Return batch of features, labels, lenghts of features,\n",
        "            and lengths of labels.\n",
        "        '''\n",
        "        # batch of input mfcc coefficients\n",
        "        # batch of output phonemes\n",
        "        # batch_mfcc = [i[0] for i in batch]\n",
        "        # batch_transcript = [i[1] for i in batch] # TODO\n",
        "        batch_mfcc = batch\n",
        "        # HINT: CHECK OUT -> pad_sequence (imported above)\n",
        "        # Also be sure to check the input format (batch_first)\n",
        "        batch_mfcc_pad = pad_sequence(batch_mfcc, batch_first=True) # TODO\n",
        "        lengths_mfcc = [len(f) for f in batch_mfcc] # TODO\n",
        "\n",
        "        #batch_transcript_pad = pad_sequence(batch_transcript, batch_first=True) # TODO\n",
        "        #lengths_transcript = [len(l) for l in batch_transcript] # TODOO\n",
        "\n",
        "        # You may apply some transformation, Time and Frequency masking, here in the collate function;\n",
        "        # Food for thought -> Why are we applying the transformation here and not in the __getitem__?\n",
        "        #                  -> Would we apply transformation on the validation set as well?\n",
        "        #                  -> Is the order of axes / dimensions as expected for the transform functions?\n",
        "\n",
        "        # Return the following values: padded features, padded labels, actual length of features, actual length of the labels\n",
        "        return batch_mfcc_pad, torch.tensor(lengths_mfcc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pt-veYcdL6Fe"
      },
      "source": [
        "### Config - Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MN82c3KpLup8"
      },
      "outputs": [],
      "source": [
        "root = '/content/11-785-s24-hw3p2/'\n",
        "\n",
        "# Feel free to add more items here\n",
        "config = {\n",
        "    \"beam_width\" : 10,\n",
        "    \"lr\"         : 2e-3,\n",
        "    \"epochs\"     : 100,\n",
        "    \"batch_size\" : 32,  # Increase if your device can handle it\n",
        "    \"classes\"    : 41,\n",
        "    \"features\"   : 27,\n",
        "    \"dropout\"    : 0.20\n",
        "}\n",
        "\n",
        "# You may pass this as a parameter to the dataset class above\n",
        "# This will help modularize your implementation\n",
        "transforms = [] # set of tranformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmuPk9J6L8dz"
      },
      "source": [
        "### Data loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_kG0gU2x4hH",
        "outputId": "3b811ec2-620e-439b-b846-8dfeecc5cc28"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# get me RAMMM!!!!\n",
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mzoYfTKu14s",
        "outputId": "d6a8080b-fb5f-412b-b272-59c8ababa1a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch size:  32\n",
            "Train dataset samples = 28539, batches = 892\n",
            "Val dataset samples = 2703, batches = 85\n",
            "Test dataset samples = 2620, batches = 82\n"
          ]
        }
      ],
      "source": [
        "# Create objects for the dataset class\n",
        "train_data = AudioDataset(root=root) #TODO\n",
        "val_data = AudioDataset(root=root, partition='dev-clean') # TODO : You can either use the same class with some modifications or make a new one :)\n",
        "test_data = AudioDatasetTest(root=root) #TODO\n",
        "\n",
        "# Do NOT forget to pass in the collate function as parameter while creating the dataloader\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = train_data,\n",
        "    #num_workers = 2,\n",
        "    batch_size  = config['batch_size'],\n",
        "    pin_memory  = True,\n",
        "    shuffle     = True,\n",
        "    collate_fn=train_data.collate_fn\n",
        ")#TODO\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = val_data,\n",
        "    #num_workers = 2,\n",
        "    batch_size  = config['batch_size'],\n",
        "    pin_memory  = True,\n",
        "    shuffle     = False,\n",
        "    collate_fn=val_data.collate_fn\n",
        ")#TODO\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = test_data,\n",
        "    #num_workers = 2,\n",
        "    batch_size  = config['batch_size'],\n",
        "    pin_memory  = True,\n",
        "    shuffle     = False,\n",
        "    collate_fn=test_data.collate_fn\n",
        ") #TODO\n",
        "\n",
        "print(\"Batch size: \", config['batch_size'])\n",
        "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
        "print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
        "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXMtwyviKaxK",
        "outputId": "905cf6dc-7656-46ac-cf47-8424d331a706"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 1606, 27]) torch.Size([32, 191]) torch.Size([32]) torch.Size([32])\n"
          ]
        }
      ],
      "source": [
        "# sanity check\n",
        "for data in train_loader:\n",
        "    x, y, lx, ly = data\n",
        "    print(x.shape, y.shape, lx.shape, ly.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSexxhdfMUzx"
      },
      "source": [
        "# NETWORK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLad4pChcuvX"
      },
      "source": [
        "## Basic\n",
        "\n",
        "This is a basic block for understanding, you can skip this and move to pBLSTM one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQhvHr71GJfq"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "class Network(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size=config['features'], hidden_size=256, num_layers=1, output_size=config['classes']):\n",
        "\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        embedding_dim = 64\n",
        "        # Adding some sort of embedding layer or feature extractor might help performance.\n",
        "        #reshape input to this (batch_size, features(channels), sequence_length) here sequences are pixels as in image\n",
        "        self.embedding = nn.Conv1d(in_channels=input_size,\n",
        "                                   out_channels=embedding_dim,\n",
        "                                   kernel_size=1)\n",
        "\n",
        "        # TODO : look up the documentation. You might need to pass some additional parameters.\n",
        "        self.lstm = nn.LSTM(input_size = embedding_dim,\n",
        "                            hidden_size = hidden_size,\n",
        "                            num_layers = num_layers,\n",
        "                            batch_first=True)\n",
        "\n",
        "        self.classification = nn.Sequential(\n",
        "            #TODO: Linear layer with in_features from the lstm module above and out_features = OUT_SIZE\n",
        "            nn.Linear(in_features=hidden_size, out_features=output_size)\n",
        "        )\n",
        "\n",
        "        self.logSoftmax = nn.LogSoftmax(dim=2)#TODO: Apply a log softmax here. Which dimension would apply it on ?\n",
        "        # (batch_size, sequence_length, phonemes(or classes))\n",
        "    def forward(self, x, lx):\n",
        "        #TODO\n",
        "        # The forward function takes 2 parameter inputs here. Why?\n",
        "        # Refer to the handout for hints\n",
        "        #(batch_size, features(channels), sequence_length)\n",
        "        z = self.embedding(x.transpose(1,2))\n",
        "\n",
        "        z = torch.relu(z).transpose(1,2)\n",
        "        z = nn.utils.rnn.pack_padded_sequence(z, lx, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        #(batch_size, sequence_length, features(channels))\n",
        "        z = self.lstm(z)[0]\n",
        "        z, lens_unpacked = nn.utils.rnn.pad_packed_sequence(z)\n",
        "\n",
        "        z = torch.relu(z)\n",
        "        print(f'Before Classification {z.shape}')\n",
        "        z = self.classification(z)\n",
        "        print(f'After classification {z.shape}')\n",
        "        z = self.logSoftmax(z)\n",
        "        print(f'After softmax {z.shape}')\n",
        "        return z, lens_unpacked"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUThsowyQdN7"
      },
      "source": [
        "## Initialize Basic Network\n",
        "(If trying out the basic Network)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "CGoiXd70tb5z",
        "outputId": "5e68f935-a127-46ef-ac3e-c2721767948d"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "model = Network().to(device)\n",
        "summary(model, x.to(device), lx.to(device)) # x and lx come from the sanity check above :)\n",
        "logits = model(x, lx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQFL_aDMkqY8",
        "outputId": "a2a0f2e0-6008-4f02-e908-0e2917854ba2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-3.7141, -3.7233, -3.7412,  ..., -3.6883, -3.6553, -3.6758],\n",
              "        [-3.7058, -3.7385, -3.7186,  ..., -3.6645, -3.6452, -3.6734],\n",
              "        [-3.7188, -3.7131, -3.7269,  ..., -3.6802, -3.6433, -3.6892],\n",
              "        ...,\n",
              "        [-3.7156, -3.7293, -3.7418,  ..., -3.6980, -3.6471, -3.7046],\n",
              "        [-3.7087, -3.7326, -3.6796,  ..., -3.6761, -3.6449, -3.7051],\n",
              "        [-3.7068, -3.7162, -3.7277,  ..., -3.6774, -3.6474, -3.7021]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "logits[0][1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-qb7wnAzCZl"
      },
      "source": [
        "## ASR Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PB6eh3gnMUzy"
      },
      "source": [
        "### Pyramid Bi-LSTM (pBLSTM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qd4BEX_yMUzz"
      },
      "outputs": [],
      "source": [
        "# Utils for network\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "class PermuteBlock(torch.nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.transpose(1, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3b5Gv4rP_Tr"
      },
      "outputs": [],
      "source": [
        "class LockedDropOut(torch.nn.Module):\n",
        "  def __init__(self, p=0.2, batch_first=True):\n",
        "    super(LockedDropOut, self).__init__()\n",
        "    self.p = p\n",
        "    self.batch_first = batch_first\n",
        "\n",
        "  def forward(self, x):\n",
        "    if not self.p or self.training:\n",
        "      return x\n",
        "\n",
        "    x, x_lengths = pad_packed_sequence(x, batch_first=self.batch_first)\n",
        "    m = x.data.new(1, x.size(1), x.size(2)).bernoulli_(1 - self.p)\n",
        "    from torch.autograd import Variable\n",
        "    mask = Variable(m, requires_grad=False)/(1-self.p)\n",
        "    mask = mask.expand_as(x)\n",
        "    x = mask*x\n",
        "    x = pack_padded_sequence(x, x_lengths, batch_first=self.batch_first, enforce_sorted=False)\n",
        "    return x\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmdyXI6KMUzz"
      },
      "outputs": [],
      "source": [
        "class pBLSTM(torch.nn.Module):\n",
        "\n",
        "    '''\n",
        "    Pyramidal BiLSTM\n",
        "    Read the write up/paper and understand the concepts and then write your implementation here.\n",
        "\n",
        "    At each step,\n",
        "    1. Pad your input if it is packed (Unpack it)\n",
        "    2. Reduce the input length dimension by concatenating feature dimension\n",
        "        (Tip: Write down the shapes and understand)\n",
        "        (i) How should  you deal with odd/even length input?\n",
        "        (ii) How should you deal with input length array (x_lens) after truncating the input?\n",
        "    3. Pack your input\n",
        "    4. Pass it into LSTM layer\n",
        "\n",
        "    To make our implementation modular, we pass 1 layer at a time.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, num_layers=3, bidirectional=True, batch_first=True, dropout=0.0, locked_dropout=0.2):\n",
        "        super(pBLSTM, self).__init__()\n",
        "\n",
        "        self.blstm = nn.LSTM(input_size = input_size,\n",
        "                            hidden_size = hidden_size,\n",
        "                            num_layers = num_layers,\n",
        "                            dropout = dropout,\n",
        "                            bidirectional=True,\n",
        "                            batch_first=True) # TODO: Initialize a single layer bidirectional LSTM with the given input_size and hidden_size\n",
        "        self.locked_dropout = LockedDropOut(locked_dropout, batch_first)\n",
        "        self.batch_first = batch_first\n",
        "\n",
        "    def forward(self, x_packed): # x_packed is a PackedSequence\n",
        "\n",
        "        # TODO: Pad Packed Sequence\n",
        "        if isinstance(x_packed, PackedSequence):\n",
        "          x, x_lens = pad_packed_sequence(x_packed, batch_first=self.batch_first)\n",
        "        else:\n",
        "          x, x_lens = x_packed, None\n",
        "        # Call self.trunc_reshape() which downsamples the time steps of x and increases the feature dimensions as mentioned above\n",
        "        x, x_lens = self.trunc_reshape(x, x_lens)\n",
        "        # self.trunc_reshape will return 2 outputs. What are they? Think about what quantites are changing.\n",
        "        if x_lens is not None:\n",
        "          x = pack_padded_sequence(x, x_lens, batch_first=self.batch_first, enforce_sorted=False)\n",
        "        # TODO: Pack Padded Sequence. What output(s) would you get?\n",
        "        # TODO: Pass the sequence through bLSTM\n",
        "        output, hidden_steps = self.blstm(x)\n",
        "        output = self.locked_dropout(output)\n",
        "        # What do you return?\n",
        "        return output, hidden_steps\n",
        "\n",
        "    def trunc_reshape(self, x, x_lens):\n",
        "        # TODO: If you have odd number of timesteps, how can you handle it? (Hint: You can exclude them)\n",
        "        # TODO: Reshape x. When reshaping x, you have to reduce number of timesteps by a downsampling factor while increasing number of features by the same factor\n",
        "        # TODO: Reduce lengths by the same downsampling factor\n",
        "        batchsize, timesteps, features = x.shape\n",
        "        if timesteps % 2 != 0:\n",
        "          x = x[:,:-1,:]\n",
        "          x_lens = x_lens - 1\n",
        "          timesteps = timesteps - 1\n",
        "\n",
        "        x_reshaped = torch.reshape(x, shape=(batchsize, timesteps//2, 2*features))\n",
        "        x_lens_reduced = torch.clamp(x_lens, max=timesteps//2, out=None)\n",
        "\n",
        "        return x_reshaped, x_lens_reduced"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3ZQ75OcMUz0"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEzw5_xmMUz0"
      },
      "outputs": [],
      "source": [
        "class Encoder(torch.nn.Module):\n",
        "    '''\n",
        "    The Encoder takes utterances as inputs and returns latent feature representations\n",
        "    '''\n",
        "    def __init__(self, input_size, encoder_hidden_size, batch_first=True, bidirectional=True, p=0.2):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.encoder_hidden_size = encoder_hidden_size\n",
        "        self.batch_first = batch_first\n",
        "        self.bidirectional = bidirectional\n",
        "        self.dropout = p\n",
        "        #TODO: You can use CNNs as Embedding layer to extract features. Keep in mind the Input dimensions and expected dimension of Pytorch CNN.\n",
        "        #Simple CNN Multilayer from HW2\n",
        "        cnn_dims = [input_size, encoder_hidden_size, encoder_hidden_size*2, encoder_hidden_size*2]\n",
        "        cnn_layers = []\n",
        "        for i in range(3):\n",
        "          cnn_layers.append(torch.nn.Conv1d(in_channels=cnn_dims[i],\n",
        "                                            out_channels=cnn_dims[i+1],\n",
        "                                            kernel_size=3,\n",
        "                                            stride=1,\n",
        "                                            padding=1,\n",
        "                                            bias=False))\n",
        "          cnn_layers.append(torch.nn.BatchNorm1d(cnn_dims[i+1]))\n",
        "          cnn_layers.append(torch.nn.GELU())\n",
        "\n",
        "        self.embedding = torch.nn.Sequential(*cnn_layers)\n",
        "        self.firstBLSTM = torch.nn.LSTM(encoder_hidden_size*2,\n",
        "                            encoder_hidden_size*2,\n",
        "                            num_layers=3,\n",
        "                            bidirectional=bidirectional,\n",
        "                            batch_first=batch_first)\n",
        "        pBLSTM_layers = [pBLSTM(input_size=encoder_hidden_size*2*2*2,\n",
        "                                hidden_size=encoder_hidden_size*2,\n",
        "                                dropout=self.dropout,\n",
        "                                batch_first=self.batch_first,\n",
        "                                locked_dropout=self.dropout) for _ in range(1)]\n",
        "        self.pBLSTMs = torch.nn.Sequential(*pBLSTM_layers)\n",
        "        # self.pBLSTMs = torch.nn.Sequential( # How many pBLSTMs are required?\n",
        "        #     # TODO: Fill this up with pBLSTMs - What should the input_size be?\n",
        "        #     # Hint: You are downsampling timesteps by a factor of 2, upsampling features by a factor of 2 and the LSTM is bidirectional)\n",
        "        #     # Optional: Dropout/Locked Dropout after each pBLSTM (Not needed for early submission)\n",
        "        #     # https://github.com/salesforce/awd-lstm-lm/blob/dfd3cb0235d2caf2847a4d53e1cbd495b781b5d2/locked_dropout.py#L5\n",
        "        #     # ...\n",
        "        #     # ...\n",
        "        # )\n",
        "\n",
        "    def forward(self, x, x_lens):\n",
        "        # Where are x and x_lens coming from? The dataloader\n",
        "        #TODO: Call the embedding layer\n",
        "        #For CNN Features must be in second dimension -  #(batch_size, features(channels), sequence_length)\n",
        "        x = torch.transpose(x, 1,2)\n",
        "        x = self.embedding.forward(x)\n",
        "        #Reshape to -> (batch_size, sequence_length, features)\n",
        "        x = torch.transpose(x,1,2)\n",
        "        # TODO: Pack Padded Sequence\n",
        "        x = pack_padded_sequence(x, x_lens.cpu(), batch_first=self.batch_first, enforce_sorted=False)\n",
        "        # TODO: Pass Sequence through the pyramidal Bi-LSTM layer\n",
        "        x, _ = self.firstBLSTM(x)\n",
        "        for pLSTM in self.pBLSTMs:\n",
        "            x, _ = pLSTM.forward(x)\n",
        "        # TODO: Pad Packed Sequence\n",
        "        encoder_outputs, encoder_lens = pad_packed_sequence(x, batch_first=self.batch_first)\n",
        "\n",
        "        # Remember the number of output(s) each function returns\n",
        "        return encoder_outputs, encoder_lens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kg82HXa3MUz1"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQIRxdNTMUz1"
      },
      "outputs": [],
      "source": [
        "class Decoder(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, embed_size, hidden_dims, p=0.2, output_size= 41):\n",
        "        super().__init__()\n",
        "        self.layers = []\n",
        "        #TODO define your MLP arch. Refer HW1P2\n",
        "        #Use Permute Block before and after BatchNorm1d() to match the size\\\n",
        "        mlp_layers = []\n",
        "        mlp_layers.append(PermuteBlock())\n",
        "        mlp_layers.append(torch.nn.BatchNorm1d(embed_size))\n",
        "        mlp_layers.append(PermuteBlock())\n",
        "        mlp_layers.append(torch.nn.Linear(embed_size,hidden_dims[0]))\n",
        "        mlp_layers.append(torch.nn.ReLU())\n",
        "        if len(hidden_dims)==1:\n",
        "            mlp_layers.append(torch.nn.Dropout(p))\n",
        "        mlp_layers.append(PermuteBlock())\n",
        "        mlp_layers.append(torch.nn.BatchNorm1d(hidden_dims[0]))\n",
        "        mlp_layers.append(PermuteBlock())\n",
        "\n",
        "        for i in range(len(hidden_dims) - 1):\n",
        "            mlp_layers.append(torch.nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
        "            mlp_layers.append(torch.nn.ReLU())\n",
        "            if i % 2 == 0:\n",
        "                mlp_layers.append(torch.nn.Dropout(p))\n",
        "            mlp_layers.append(PermuteBlock())\n",
        "            mlp_layers.append(torch.nn.BatchNorm1d(hidden_dims[i+1]))\n",
        "            mlp_layers.append(PermuteBlock())\n",
        "        mlp_layers.append(torch.nn.Linear(hidden_dims[-1],output_size))\n",
        "\n",
        "        self.mlp = torch.nn.Sequential(*mlp_layers)\n",
        "        self.softmax = torch.nn.LogSoftmax(dim=2)\n",
        "\n",
        "    def forward(self, encoder_out):\n",
        "        #TODO call your MLP\n",
        "        #TODO Think what should be the final output of the decoder for the classification\n",
        "        out = self.mlp.forward(encoder_out)\n",
        "        out = self.softmax(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmHf6pFiMUz1"
      },
      "outputs": [],
      "source": [
        "import torchaudio.transforms as tat\n",
        "class ASRModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_dims, encoder_hidden_size=64, dropout=0.2, embed_size= 4*64, output_size= len(PHONEMES)):\n",
        "        super().__init__()\n",
        "\n",
        "        self.augmentations  = torch.nn.Sequential(\n",
        "            #TODO Add Time Masking/ Frequency Masking\n",
        "            tat.TimeMasking(dropout * 100),\n",
        "            tat.FrequencyMasking(dropout * 100)\n",
        "            #Hint: See how to use PermuteBlock() function defined above\n",
        "        )\n",
        "        self.encoder        = Encoder(input_size, encoder_hidden_size, p=dropout) # TODO: Initialize Encoder\n",
        "        self.decoder        = Decoder(embed_size, hidden_dims)                    # TODO: Initialize Decoder\n",
        "\n",
        "    def forward(self, x, lengths_x):\n",
        "\n",
        "        if self.training:\n",
        "            x = self.augmentations(x)\n",
        "\n",
        "        encoder_out, encoder_lens   = self.encoder(x, lengths_x)\n",
        "        decoder_out                 = self.decoder(encoder_out)\n",
        "\n",
        "        return decoder_out, encoder_lens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EV7DMPDoMUz2"
      },
      "source": [
        "## Initialize ASR Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oaaDsnnLMUz2",
        "outputId": "bf57801b-ce24-4d9a-c28a-7d87304a2dc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ASRModel(\n",
            "  (augmentations): Sequential(\n",
            "    (0): TimeMasking()\n",
            "    (1): FrequencyMasking()\n",
            "  )\n",
            "  (encoder): Encoder(\n",
            "    (embedding): Sequential(\n",
            "      (0): Conv1d(27, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
            "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): GELU(approximate='none')\n",
            "      (3): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
            "      (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): GELU(approximate='none')\n",
            "      (6): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
            "      (7): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (8): GELU(approximate='none')\n",
            "    )\n",
            "    (firstBLSTM): LSTM(128, 128, num_layers=3, batch_first=True, bidirectional=True)\n",
            "    (pBLSTMs): Sequential(\n",
            "      (0): pBLSTM(\n",
            "        (blstm): LSTM(512, 128, num_layers=3, batch_first=True, dropout=0.2, bidirectional=True)\n",
            "        (locked_dropout): LockedDropOut()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (mlp): Sequential(\n",
            "      (0): PermuteBlock()\n",
            "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): PermuteBlock()\n",
            "      (3): Linear(in_features=256, out_features=512, bias=True)\n",
            "      (4): ReLU()\n",
            "      (5): PermuteBlock()\n",
            "      (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (7): PermuteBlock()\n",
            "      (8): Linear(in_features=512, out_features=128, bias=True)\n",
            "      (9): ReLU()\n",
            "      (10): Dropout(p=0.2, inplace=False)\n",
            "      (11): PermuteBlock()\n",
            "      (12): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (13): PermuteBlock()\n",
            "      (14): Linear(in_features=128, out_features=41, bias=True)\n",
            "    )\n",
            "    (softmax): LogSoftmax(dim=2)\n",
            "  )\n",
            ")\n",
            "========================================================================================================\n",
            "                                                    Kernel Shape  \\\n",
            "Layer                                                              \n",
            "0_augmentations.TimeMasking_0                                  -   \n",
            "1_augmentations.FrequencyMasking_1                             -   \n",
            "2_encoder.embedding.Conv1d_0                         [27, 64, 3]   \n",
            "3_encoder.embedding.BatchNorm1d_1                           [64]   \n",
            "4_encoder.embedding.GELU_2                                     -   \n",
            "5_encoder.embedding.Conv1d_3                        [64, 128, 3]   \n",
            "6_encoder.embedding.BatchNorm1d_4                          [128]   \n",
            "7_encoder.embedding.GELU_5                                     -   \n",
            "8_encoder.embedding.Conv1d_6                       [128, 128, 3]   \n",
            "9_encoder.embedding.BatchNorm1d_7                          [128]   \n",
            "10_encoder.embedding.GELU_8                                    -   \n",
            "11_encoder.LSTM_firstBLSTM                                     -   \n",
            "12_encoder.pBLSTMs.0.LSTM_blstm                                -   \n",
            "13_encoder.pBLSTMs.0.LockedDropOut_locked_dropout              -   \n",
            "14_decoder.mlp.PermuteBlock_0                                  -   \n",
            "15_decoder.mlp.BatchNorm1d_1                               [256]   \n",
            "16_decoder.mlp.PermuteBlock_2                                  -   \n",
            "17_decoder.mlp.Linear_3                               [256, 512]   \n",
            "18_decoder.mlp.ReLU_4                                          -   \n",
            "19_decoder.mlp.PermuteBlock_5                                  -   \n",
            "20_decoder.mlp.BatchNorm1d_6                               [512]   \n",
            "21_decoder.mlp.PermuteBlock_7                                  -   \n",
            "22_decoder.mlp.Linear_8                               [512, 128]   \n",
            "23_decoder.mlp.ReLU_9                                          -   \n",
            "24_decoder.mlp.Dropout_10                                      -   \n",
            "25_decoder.mlp.PermuteBlock_11                                 -   \n",
            "26_decoder.mlp.BatchNorm1d_12                              [128]   \n",
            "27_decoder.mlp.PermuteBlock_13                                 -   \n",
            "28_decoder.mlp.Linear_14                               [128, 41]   \n",
            "29_decoder.LogSoftmax_softmax                                  -   \n",
            "\n",
            "                                                      Output Shape     Params  \\\n",
            "Layer                                                                           \n",
            "0_augmentations.TimeMasking_0                       [64, 1618, 27]          -   \n",
            "1_augmentations.FrequencyMasking_1                  [64, 1618, 27]          -   \n",
            "2_encoder.embedding.Conv1d_0                        [64, 64, 1618]     5.184k   \n",
            "3_encoder.embedding.BatchNorm1d_1                   [64, 64, 1618]      128.0   \n",
            "4_encoder.embedding.GELU_2                          [64, 64, 1618]          -   \n",
            "5_encoder.embedding.Conv1d_3                       [64, 128, 1618]    24.576k   \n",
            "6_encoder.embedding.BatchNorm1d_4                  [64, 128, 1618]      256.0   \n",
            "7_encoder.embedding.GELU_5                         [64, 128, 1618]          -   \n",
            "8_encoder.embedding.Conv1d_6                       [64, 128, 1618]    49.152k   \n",
            "9_encoder.embedding.BatchNorm1d_7                  [64, 128, 1618]      256.0   \n",
            "10_encoder.embedding.GELU_8                        [64, 128, 1618]          -   \n",
            "11_encoder.LSTM_firstBLSTM                            [76495, 256]   1.05472M   \n",
            "12_encoder.pBLSTMs.0.LSTM_blstm                       [47658, 256]  1.447936M   \n",
            "13_encoder.pBLSTMs.0.LockedDropOut_locked_dropout     [47658, 256]          -   \n",
            "14_decoder.mlp.PermuteBlock_0                       [64, 256, 809]          -   \n",
            "15_decoder.mlp.BatchNorm1d_1                        [64, 256, 809]      512.0   \n",
            "16_decoder.mlp.PermuteBlock_2                       [64, 809, 256]          -   \n",
            "17_decoder.mlp.Linear_3                             [64, 809, 512]   131.584k   \n",
            "18_decoder.mlp.ReLU_4                               [64, 809, 512]          -   \n",
            "19_decoder.mlp.PermuteBlock_5                       [64, 512, 809]          -   \n",
            "20_decoder.mlp.BatchNorm1d_6                        [64, 512, 809]     1.024k   \n",
            "21_decoder.mlp.PermuteBlock_7                       [64, 809, 512]          -   \n",
            "22_decoder.mlp.Linear_8                             [64, 809, 128]    65.664k   \n",
            "23_decoder.mlp.ReLU_9                               [64, 809, 128]          -   \n",
            "24_decoder.mlp.Dropout_10                           [64, 809, 128]          -   \n",
            "25_decoder.mlp.PermuteBlock_11                      [64, 128, 809]          -   \n",
            "26_decoder.mlp.BatchNorm1d_12                       [64, 128, 809]      256.0   \n",
            "27_decoder.mlp.PermuteBlock_13                      [64, 809, 128]          -   \n",
            "28_decoder.mlp.Linear_14                             [64, 809, 41]     5.289k   \n",
            "29_decoder.LogSoftmax_softmax                        [64, 809, 41]          -   \n",
            "\n",
            "                                                    Mult-Adds  \n",
            "Layer                                                          \n",
            "0_augmentations.TimeMasking_0                               -  \n",
            "1_augmentations.FrequencyMasking_1                          -  \n",
            "2_encoder.embedding.Conv1d_0                        8.387712M  \n",
            "3_encoder.embedding.BatchNorm1d_1                        64.0  \n",
            "4_encoder.embedding.GELU_2                                  -  \n",
            "5_encoder.embedding.Conv1d_3                       39.763968M  \n",
            "6_encoder.embedding.BatchNorm1d_4                       128.0  \n",
            "7_encoder.embedding.GELU_5                                  -  \n",
            "8_encoder.embedding.Conv1d_6                       79.527936M  \n",
            "9_encoder.embedding.BatchNorm1d_7                       128.0  \n",
            "10_encoder.embedding.GELU_8                                 -  \n",
            "11_encoder.LSTM_firstBLSTM                          1.048576M  \n",
            "12_encoder.pBLSTMs.0.LSTM_blstm                     1.441792M  \n",
            "13_encoder.pBLSTMs.0.LockedDropOut_locked_dropout           -  \n",
            "14_decoder.mlp.PermuteBlock_0                               -  \n",
            "15_decoder.mlp.BatchNorm1d_1                            256.0  \n",
            "16_decoder.mlp.PermuteBlock_2                               -  \n",
            "17_decoder.mlp.Linear_3                              131.072k  \n",
            "18_decoder.mlp.ReLU_4                                       -  \n",
            "19_decoder.mlp.PermuteBlock_5                               -  \n",
            "20_decoder.mlp.BatchNorm1d_6                            512.0  \n",
            "21_decoder.mlp.PermuteBlock_7                               -  \n",
            "22_decoder.mlp.Linear_8                               65.536k  \n",
            "23_decoder.mlp.ReLU_9                                       -  \n",
            "24_decoder.mlp.Dropout_10                                   -  \n",
            "25_decoder.mlp.PermuteBlock_11                              -  \n",
            "26_decoder.mlp.BatchNorm1d_12                           128.0  \n",
            "27_decoder.mlp.PermuteBlock_13                              -  \n",
            "28_decoder.mlp.Linear_14                               5.248k  \n",
            "29_decoder.LogSoftmax_softmax                               -  \n",
            "--------------------------------------------------------------------------------------------------------\n",
            "                           Totals\n",
            "Total params            2.786537M\n",
            "Trainable params        2.786537M\n",
            "Non-trainable params          0.0\n",
            "Mult-Adds             130.373056M\n",
            "========================================================================================================\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "repr_error": "unhashable type: 'list'",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-eeb62b6b-fce6-4cb2-95de-5b07ad1cea07\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Kernel Shape</th>\n",
              "      <th>Output Shape</th>\n",
              "      <th>Params</th>\n",
              "      <th>Mult-Adds</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Layer</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_augmentations.TimeMasking_0</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 1618, 27]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_augmentations.FrequencyMasking_1</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 1618, 27]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2_encoder.embedding.Conv1d_0</th>\n",
              "      <td>[27, 64, 3]</td>\n",
              "      <td>[64, 64, 1618]</td>\n",
              "      <td>5184.0</td>\n",
              "      <td>8387712.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3_encoder.embedding.BatchNorm1d_1</th>\n",
              "      <td>[64]</td>\n",
              "      <td>[64, 64, 1618]</td>\n",
              "      <td>128.0</td>\n",
              "      <td>64.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4_encoder.embedding.GELU_2</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 64, 1618]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5_encoder.embedding.Conv1d_3</th>\n",
              "      <td>[64, 128, 3]</td>\n",
              "      <td>[64, 128, 1618]</td>\n",
              "      <td>24576.0</td>\n",
              "      <td>39763968.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6_encoder.embedding.BatchNorm1d_4</th>\n",
              "      <td>[128]</td>\n",
              "      <td>[64, 128, 1618]</td>\n",
              "      <td>256.0</td>\n",
              "      <td>128.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7_encoder.embedding.GELU_5</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 128, 1618]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8_encoder.embedding.Conv1d_6</th>\n",
              "      <td>[128, 128, 3]</td>\n",
              "      <td>[64, 128, 1618]</td>\n",
              "      <td>49152.0</td>\n",
              "      <td>79527936.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9_encoder.embedding.BatchNorm1d_7</th>\n",
              "      <td>[128]</td>\n",
              "      <td>[64, 128, 1618]</td>\n",
              "      <td>256.0</td>\n",
              "      <td>128.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10_encoder.embedding.GELU_8</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 128, 1618]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11_encoder.LSTM_firstBLSTM</th>\n",
              "      <td>-</td>\n",
              "      <td>[76495, 256]</td>\n",
              "      <td>1054720.0</td>\n",
              "      <td>1048576.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12_encoder.pBLSTMs.0.LSTM_blstm</th>\n",
              "      <td>-</td>\n",
              "      <td>[47658, 256]</td>\n",
              "      <td>1447936.0</td>\n",
              "      <td>1441792.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13_encoder.pBLSTMs.0.LockedDropOut_locked_dropout</th>\n",
              "      <td>-</td>\n",
              "      <td>[47658, 256]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14_decoder.mlp.PermuteBlock_0</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 256, 809]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15_decoder.mlp.BatchNorm1d_1</th>\n",
              "      <td>[256]</td>\n",
              "      <td>[64, 256, 809]</td>\n",
              "      <td>512.0</td>\n",
              "      <td>256.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16_decoder.mlp.PermuteBlock_2</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 809, 256]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17_decoder.mlp.Linear_3</th>\n",
              "      <td>[256, 512]</td>\n",
              "      <td>[64, 809, 512]</td>\n",
              "      <td>131584.0</td>\n",
              "      <td>131072.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18_decoder.mlp.ReLU_4</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 809, 512]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19_decoder.mlp.PermuteBlock_5</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 512, 809]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20_decoder.mlp.BatchNorm1d_6</th>\n",
              "      <td>[512]</td>\n",
              "      <td>[64, 512, 809]</td>\n",
              "      <td>1024.0</td>\n",
              "      <td>512.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21_decoder.mlp.PermuteBlock_7</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 809, 512]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22_decoder.mlp.Linear_8</th>\n",
              "      <td>[512, 128]</td>\n",
              "      <td>[64, 809, 128]</td>\n",
              "      <td>65664.0</td>\n",
              "      <td>65536.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23_decoder.mlp.ReLU_9</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 809, 128]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24_decoder.mlp.Dropout_10</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 809, 128]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25_decoder.mlp.PermuteBlock_11</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 128, 809]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26_decoder.mlp.BatchNorm1d_12</th>\n",
              "      <td>[128]</td>\n",
              "      <td>[64, 128, 809]</td>\n",
              "      <td>256.0</td>\n",
              "      <td>128.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27_decoder.mlp.PermuteBlock_13</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 809, 128]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28_decoder.mlp.Linear_14</th>\n",
              "      <td>[128, 41]</td>\n",
              "      <td>[64, 809, 41]</td>\n",
              "      <td>5289.0</td>\n",
              "      <td>5248.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29_decoder.LogSoftmax_softmax</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 809, 41]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eeb62b6b-fce6-4cb2-95de-5b07ad1cea07')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-eeb62b6b-fce6-4cb2-95de-5b07ad1cea07 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-eeb62b6b-fce6-4cb2-95de-5b07ad1cea07');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3b055b5e-69d2-4b27-bd58-b26b4768df43\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3b055b5e-69d2-4b27-bd58-b26b4768df43')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3b055b5e-69d2-4b27-bd58-b26b4768df43 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                    Kernel Shape  \\\n",
              "Layer                                                              \n",
              "0_augmentations.TimeMasking_0                                  -   \n",
              "1_augmentations.FrequencyMasking_1                             -   \n",
              "2_encoder.embedding.Conv1d_0                         [27, 64, 3]   \n",
              "3_encoder.embedding.BatchNorm1d_1                           [64]   \n",
              "4_encoder.embedding.GELU_2                                     -   \n",
              "5_encoder.embedding.Conv1d_3                        [64, 128, 3]   \n",
              "6_encoder.embedding.BatchNorm1d_4                          [128]   \n",
              "7_encoder.embedding.GELU_5                                     -   \n",
              "8_encoder.embedding.Conv1d_6                       [128, 128, 3]   \n",
              "9_encoder.embedding.BatchNorm1d_7                          [128]   \n",
              "10_encoder.embedding.GELU_8                                    -   \n",
              "11_encoder.LSTM_firstBLSTM                                     -   \n",
              "12_encoder.pBLSTMs.0.LSTM_blstm                                -   \n",
              "13_encoder.pBLSTMs.0.LockedDropOut_locked_dropout              -   \n",
              "14_decoder.mlp.PermuteBlock_0                                  -   \n",
              "15_decoder.mlp.BatchNorm1d_1                               [256]   \n",
              "16_decoder.mlp.PermuteBlock_2                                  -   \n",
              "17_decoder.mlp.Linear_3                               [256, 512]   \n",
              "18_decoder.mlp.ReLU_4                                          -   \n",
              "19_decoder.mlp.PermuteBlock_5                                  -   \n",
              "20_decoder.mlp.BatchNorm1d_6                               [512]   \n",
              "21_decoder.mlp.PermuteBlock_7                                  -   \n",
              "22_decoder.mlp.Linear_8                               [512, 128]   \n",
              "23_decoder.mlp.ReLU_9                                          -   \n",
              "24_decoder.mlp.Dropout_10                                      -   \n",
              "25_decoder.mlp.PermuteBlock_11                                 -   \n",
              "26_decoder.mlp.BatchNorm1d_12                              [128]   \n",
              "27_decoder.mlp.PermuteBlock_13                                 -   \n",
              "28_decoder.mlp.Linear_14                               [128, 41]   \n",
              "29_decoder.LogSoftmax_softmax                                  -   \n",
              "\n",
              "                                                      Output Shape     Params  \\\n",
              "Layer                                                                           \n",
              "0_augmentations.TimeMasking_0                       [64, 1618, 27]        NaN   \n",
              "1_augmentations.FrequencyMasking_1                  [64, 1618, 27]        NaN   \n",
              "2_encoder.embedding.Conv1d_0                        [64, 64, 1618]     5184.0   \n",
              "3_encoder.embedding.BatchNorm1d_1                   [64, 64, 1618]      128.0   \n",
              "4_encoder.embedding.GELU_2                          [64, 64, 1618]        NaN   \n",
              "5_encoder.embedding.Conv1d_3                       [64, 128, 1618]    24576.0   \n",
              "6_encoder.embedding.BatchNorm1d_4                  [64, 128, 1618]      256.0   \n",
              "7_encoder.embedding.GELU_5                         [64, 128, 1618]        NaN   \n",
              "8_encoder.embedding.Conv1d_6                       [64, 128, 1618]    49152.0   \n",
              "9_encoder.embedding.BatchNorm1d_7                  [64, 128, 1618]      256.0   \n",
              "10_encoder.embedding.GELU_8                        [64, 128, 1618]        NaN   \n",
              "11_encoder.LSTM_firstBLSTM                            [76495, 256]  1054720.0   \n",
              "12_encoder.pBLSTMs.0.LSTM_blstm                       [47658, 256]  1447936.0   \n",
              "13_encoder.pBLSTMs.0.LockedDropOut_locked_dropout     [47658, 256]        NaN   \n",
              "14_decoder.mlp.PermuteBlock_0                       [64, 256, 809]        NaN   \n",
              "15_decoder.mlp.BatchNorm1d_1                        [64, 256, 809]      512.0   \n",
              "16_decoder.mlp.PermuteBlock_2                       [64, 809, 256]        NaN   \n",
              "17_decoder.mlp.Linear_3                             [64, 809, 512]   131584.0   \n",
              "18_decoder.mlp.ReLU_4                               [64, 809, 512]        NaN   \n",
              "19_decoder.mlp.PermuteBlock_5                       [64, 512, 809]        NaN   \n",
              "20_decoder.mlp.BatchNorm1d_6                        [64, 512, 809]     1024.0   \n",
              "21_decoder.mlp.PermuteBlock_7                       [64, 809, 512]        NaN   \n",
              "22_decoder.mlp.Linear_8                             [64, 809, 128]    65664.0   \n",
              "23_decoder.mlp.ReLU_9                               [64, 809, 128]        NaN   \n",
              "24_decoder.mlp.Dropout_10                           [64, 809, 128]        NaN   \n",
              "25_decoder.mlp.PermuteBlock_11                      [64, 128, 809]        NaN   \n",
              "26_decoder.mlp.BatchNorm1d_12                       [64, 128, 809]      256.0   \n",
              "27_decoder.mlp.PermuteBlock_13                      [64, 809, 128]        NaN   \n",
              "28_decoder.mlp.Linear_14                             [64, 809, 41]     5289.0   \n",
              "29_decoder.LogSoftmax_softmax                        [64, 809, 41]        NaN   \n",
              "\n",
              "                                                    Mult-Adds  \n",
              "Layer                                                          \n",
              "0_augmentations.TimeMasking_0                             NaN  \n",
              "1_augmentations.FrequencyMasking_1                        NaN  \n",
              "2_encoder.embedding.Conv1d_0                        8387712.0  \n",
              "3_encoder.embedding.BatchNorm1d_1                        64.0  \n",
              "4_encoder.embedding.GELU_2                                NaN  \n",
              "5_encoder.embedding.Conv1d_3                       39763968.0  \n",
              "6_encoder.embedding.BatchNorm1d_4                       128.0  \n",
              "7_encoder.embedding.GELU_5                                NaN  \n",
              "8_encoder.embedding.Conv1d_6                       79527936.0  \n",
              "9_encoder.embedding.BatchNorm1d_7                       128.0  \n",
              "10_encoder.embedding.GELU_8                               NaN  \n",
              "11_encoder.LSTM_firstBLSTM                          1048576.0  \n",
              "12_encoder.pBLSTMs.0.LSTM_blstm                     1441792.0  \n",
              "13_encoder.pBLSTMs.0.LockedDropOut_locked_dropout         NaN  \n",
              "14_decoder.mlp.PermuteBlock_0                             NaN  \n",
              "15_decoder.mlp.BatchNorm1d_1                            256.0  \n",
              "16_decoder.mlp.PermuteBlock_2                             NaN  \n",
              "17_decoder.mlp.Linear_3                              131072.0  \n",
              "18_decoder.mlp.ReLU_4                                     NaN  \n",
              "19_decoder.mlp.PermuteBlock_5                             NaN  \n",
              "20_decoder.mlp.BatchNorm1d_6                            512.0  \n",
              "21_decoder.mlp.PermuteBlock_7                             NaN  \n",
              "22_decoder.mlp.Linear_8                               65536.0  \n",
              "23_decoder.mlp.ReLU_9                                     NaN  \n",
              "24_decoder.mlp.Dropout_10                                 NaN  \n",
              "25_decoder.mlp.PermuteBlock_11                            NaN  \n",
              "26_decoder.mlp.BatchNorm1d_12                           128.0  \n",
              "27_decoder.mlp.PermuteBlock_13                            NaN  \n",
              "28_decoder.mlp.Linear_14                               5248.0  \n",
              "29_decoder.LogSoftmax_softmax                             NaN  "
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = ASRModel(\n",
        "    input_size  = config['features'], #TODO,\n",
        "    hidden_dims = [512,128], #embed_size  =    #TODO\n",
        "    dropout = config['dropout'],\n",
        "    output_size = len(PHONEMES)\n",
        ").to(device)\n",
        "print(model)\n",
        "summary(model, x.to(device), lx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VL_t8TQNz7LS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6IcVCMYz8QQ"
      },
      "source": [
        "## Network2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uF_ImU1C0BHP"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ASRNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, embed_dim, hidden_dim, out_size, dropout):\n",
        "\n",
        "        super(ASRNetwork, self).__init__()\n",
        "        self.augmentations  = torch.nn.Sequential(\n",
        "            #TODO Add Time Masking/ Frequency Masking\n",
        "            tat.TimeMasking(dropout * 100),\n",
        "            tat.FrequencyMasking(dropout * 100)\n",
        "            #Hint: See how to use PermuteBlock() function defined above\n",
        "        )\n",
        "\n",
        "        self.embedding = nn.Sequential(nn.Conv1d(in_channels = input_size,\n",
        "                                                 out_channels = embed_dim,\n",
        "                                                 bias = False,\n",
        "                                                 kernel_size = 1,\n",
        "                                                 padding = 0,\n",
        "                                                 stride = 1),\n",
        "                                       nn.BatchNorm1d(embed_dim),\n",
        "                                       nn.GELU(),\n",
        "                                       nn.Conv1d(in_channels = embed_dim,\n",
        "                                                 out_channels = embed_dim,\n",
        "                                                 bias = False,\n",
        "                                                 kernel_size = 3,\n",
        "                                                 padding = 1,\n",
        "                                                 stride = 1,\n",
        "                                                 groups = embed_dim),\n",
        "                                       nn.BatchNorm1d(embed_dim),\n",
        "                                       nn.GELU(),\n",
        "                                       nn.Conv1d(in_channels = embed_dim,\n",
        "                                                 out_channels = embed_dim,\n",
        "                                                 bias = False,\n",
        "                                                 kernel_size = 3,\n",
        "                                                 padding = 1,\n",
        "                                                 stride = 1,\n",
        "                                                 groups = embed_dim),\n",
        "                                       nn.BatchNorm1d(embed_dim),\n",
        "                                       nn.GELU(),\n",
        "                                       nn.Conv1d(in_channels = embed_dim,\n",
        "                                                 out_channels = embed_dim,\n",
        "                                                 bias = False,\n",
        "                                                 kernel_size = 3,\n",
        "                                                 padding = 1,\n",
        "                                                 stride = 1,\n",
        "                                                 groups = embed_dim),\n",
        "                                       nn.BatchNorm1d(embed_dim),\n",
        "                                       nn.GELU(),\n",
        "                                       nn.Conv1d(in_channels = embed_dim,\n",
        "                                                 out_channels = embed_dim,\n",
        "                                                 bias = False,\n",
        "                                                 kernel_size = 3,\n",
        "                                                 padding = 1,\n",
        "                                                 stride = 1,\n",
        "                                                 groups = embed_dim),\n",
        "                                       nn.BatchNorm1d(embed_dim),\n",
        "                                       nn.GELU(),\n",
        "                                       nn.Conv1d(in_channels = embed_dim,\n",
        "                                                 out_channels = embed_dim,\n",
        "                                                 bias = False,\n",
        "                                                 kernel_size = 3,\n",
        "                                                 padding = 1,\n",
        "                                                 stride = 1,\n",
        "                                                 groups = embed_dim),\n",
        "                                       nn.BatchNorm1d(embed_dim),\n",
        "                                       nn.GELU(),\n",
        "                                       nn.Dropout(dropout))\n",
        "        self.lstm = nn.LSTM(input_size = embed_dim, hidden_size = hidden_dim, num_layers = 4, bidirectional = True)\n",
        "\n",
        "        self.classification = nn.Sequential(\n",
        "            nn.Linear((hidden_dim * 2), 2048),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(2048, 41))\n",
        "\n",
        "        self.logSoftmax = nn.LogSoftmax(dim = 2)\n",
        "\n",
        "    def forward(self, x, lx):\n",
        "        if self.training:\n",
        "            x = self.augmentations(x)\n",
        "        out = torch.permute(x, (0,2,1))\n",
        "        out = self.embedding(out)\n",
        "        out = torch.permute(out, (0,2,1))\n",
        "        packed_input = pack_padded_sequence(out, lx, enforce_sorted=False, batch_first = True)\n",
        "\n",
        "        lstm_out, hidden_dims = self.lstm(packed_input)\n",
        "\n",
        "\n",
        "        lstm_pad_pack, lx  = pad_packed_sequence(lstm_out, batch_first = True)\n",
        "\n",
        "        out = self.classification(lstm_pad_pack)\n",
        "        out = self.logSoftmax(out)\n",
        "\n",
        "        out = torch.permute(out, (1,0,2))\n",
        "\n",
        "        return out, lx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvnQASvJjToI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmX2P7iTjUSx"
      },
      "source": [
        "## Network3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 877
        },
        "id": "pXfPl0Ug0IOX",
        "outputId": "2f06d600-1069-44fe-ea10-1dcf92b34673"
      },
      "outputs": [],
      "source": [
        "model = ASRNetwork(input_size=config['features'],\n",
        "                   embed_dim=128,\n",
        "                   hidden_dim=512,\n",
        "                   out_size=41,\n",
        "                   dropout=0.25).to(device)\n",
        "print(model)\n",
        "summary(model, x.to(device), lx) # x and lx come from the sanity check above :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBwunYpyugFg"
      },
      "source": [
        "# Training Config\n",
        "Initialize Loss Criterion, Optimizer, CTC Beam Decoder, Scheduler, Scaler (Mixed-Precision), etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGoozH2nd6KB"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "\n",
        "\n",
        "criterion = torch.nn.CTCLoss(zero_infinity=True)# Define CTC loss as the criterion. How would the losses be reduced?\n",
        "# CTC Loss: https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html\n",
        "# Refer to the handout for hints\n",
        "\n",
        "optimizer =  torch.optim.AdamW(model.parameters(), config['lr'], weight_decay=1e-5) # What goes in here?\n",
        "\n",
        "# Declare the decoder. Use the CTC Beam Decoder to decode phonemes\n",
        "# CTC Beam Decoder Doc: https://github.com/parlance/ctcdecode\n",
        "decoder = CTCBeamDecoder(LABELS, beam_width=config['beam_width'], log_probs_input=True)#TODO\n",
        "\n",
        "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config['epochs'])#TODO\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.8, patience=2, min_lr=1e-7)\n",
        "\n",
        "# Mixed Precision, if you need it\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jmc6_4eWL2Xp"
      },
      "source": [
        "# Decode Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHjnCDddL36E"
      },
      "outputs": [],
      "source": [
        "def decode_prediction(output, output_lens, decoder, PHONEME_MAP= LABELS):\n",
        "    if output.shape[0] != config['batch_size']:\n",
        "        output = torch.transpose(output, 0, 1)\n",
        "    # TODO: look at docs for CTC.decoder and find out what is returned here. Check the shape of output and expected shape in decode.\n",
        "    beam_results, beam_scores, timesteps, out_lens = decoder.decode(output, seq_lens= output_lens) #lengths - list of lengths\n",
        "\n",
        "    pred_strings                    = []\n",
        "\n",
        "    for i in range(output_lens.shape[0]):\n",
        "        #TODO: Create the prediction from the output of decoder.decode. Don't forget to map it using PHONEMES_MAP.\n",
        "        beam = beam_results[i,0,:out_lens[i,0]]\n",
        "        pred_string = []\n",
        "        pred_string = [PHONEME_MAP[i] for i in beam]\n",
        "        pred_strings.append(pred_string)\n",
        "\n",
        "    return pred_strings\n",
        "\n",
        "def calculate_levenshtein(output, label, output_lens, label_lens, decoder, PHONEME_MAP= LABELS): # y - sequence of integers\n",
        "\n",
        "    dist            = 0\n",
        "    batch_size      = label.shape[0]\n",
        "\n",
        "    pred_strings    = decode_prediction(output, output_lens, decoder, PHONEME_MAP)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        # TODO: Get predicted string and label string for each element in the batch\n",
        "        pred_string =pred_strings[i] #TODO\n",
        "        label_beam = label[i, 0:label_lens[i]]\n",
        "        label_string = [PHONEME_MAP[k] for k in label_beam] #TODO\n",
        "        dist += Levenshtein.distance(pred_string, label_string)\n",
        "\n",
        "    dist /= batch_size # TODO: Uncomment this, but think about why we are doing this\n",
        "    return dist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Qk9iZud1LXT"
      },
      "source": [
        "# Test Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "GnTLL-5gMBrY",
        "outputId": "a54045a1-3686-4c17-8a41-c92fa1e9f278"
      },
      "outputs": [],
      "source": [
        "# test code to check shapes\n",
        "\n",
        "model.eval()\n",
        "for i, data in enumerate(val_loader, 0):\n",
        "    x, y, lx, ly = data\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    h, lh = model(x, lx)\n",
        "    print(h.shape)\n",
        "    #h = torch.permute(h, (1, 0, 2))\n",
        "    print(h.shape, y.shape)\n",
        "    loss = criterion(h, y, lh, ly)\n",
        "    print(loss)\n",
        "\n",
        "    print(calculate_levenshtein(h, y, lx, ly, decoder, LABELS))\n",
        "\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd5aNaLVoR_g"
      },
      "source": [
        "# WandB\n",
        "\n",
        "You will need to fetch your api key from wandb.ai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiDduMaDIARE",
        "outputId": "9c9570a8-7c4c-46b8-fad2-8a0bcf440a92"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.login(key=\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "4s52yBOvICPZ",
        "outputId": "94ffd2ce-5628-42fd-a697-e13bd2695c46"
      },
      "outputs": [],
      "source": [
        "run = wandb.init(\n",
        "    name = \"ASR\", ## Wandb creates random run names if you skip this field\n",
        "    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "    #id = 'ub6mhzz5',### Insert specific run id here if you want to resume a previous run\n",
        "    #resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "    project = \"\", ### Project should be created in your wandb account\n",
        "    config = config ### Wandb Config for your run\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fLLj5KIMMOe"
      },
      "source": [
        "# Train Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ri87MAdhMUz5"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train_model(model, train_loader, criterion, optimizer):\n",
        "\n",
        "    model.train()\n",
        "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    for i, data in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x, y, lx, ly = data\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            h, lh = model(x, lx)\n",
        "            #h = torch.permute(h, (1, 0, 2))\n",
        "            loss = criterion(h, y, lh, ly)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "            lr=\"{:.06f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "\n",
        "        batch_bar.update() # Update tqdm bar\n",
        "\n",
        "        # Another couple things you need for FP16.\n",
        "        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
        "        scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
        "        scaler.update() # This is something added just for FP16\n",
        "\n",
        "        del x, y, lx, ly, h, lh, loss\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close() # You need this to close the tqdm bar\n",
        "\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "\n",
        "def validate_model(model, val_loader, decoder, phoneme_map= LABELS):\n",
        "\n",
        "    model.eval()\n",
        "    batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
        "\n",
        "    total_loss = 0\n",
        "    vdist = 0\n",
        "\n",
        "    for i, data in enumerate(val_loader):\n",
        "\n",
        "        x, y, lx, ly = data\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            h, lh = model(x, lx)\n",
        "            #h = torch.permute(h, (1, 0, 2))\n",
        "            loss = criterion(h, y, lh, ly)\n",
        "\n",
        "        total_loss += float(loss)\n",
        "        vdist += calculate_levenshtein(h, y, lh, ly, decoder, phoneme_map)\n",
        "        #vdist += calculate_levenshtein(torch.permute(h, (1, 0, 2)), y, lh, ly, decoder, phoneme_map)\n",
        "\n",
        "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(total_loss / (i + 1))), dist=\"{:.04f}\".format(float(vdist / (i + 1))))\n",
        "\n",
        "        batch_bar.update()\n",
        "\n",
        "        del x, y, lx, ly, h, lh, loss\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close()\n",
        "    total_loss = total_loss/len(val_loader)\n",
        "    val_dist = vdist/len(val_loader)\n",
        "    return total_loss, val_dist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpYExu4vT4_g"
      },
      "source": [
        "## Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "husa5_EYMUz6"
      },
      "outputs": [],
      "source": [
        "def save_model(model, optimizer, scheduler, metric, epoch, path):\n",
        "    torch.save(\n",
        "        {'model_state_dict'         : model.state_dict(),\n",
        "         'optimizer_state_dict'     : optimizer.state_dict(),\n",
        "         'scheduler_state_dict'     : scheduler.state_dict(),\n",
        "         metric[0]                  : metric[1],\n",
        "         'epoch'                    : epoch},\n",
        "         path\n",
        "    )\n",
        "\n",
        "def load_model(path, model, metric= 'valid_acc', optimizer= None, scheduler= None):\n",
        "\n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    if optimizer != None:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    if scheduler != None:\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "\n",
        "    epoch   = checkpoint['epoch']\n",
        "    metric  = checkpoint[metric]\n",
        "\n",
        "    return [model, optimizer, scheduler, epoch, metric]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tExvyl1BIdMC"
      },
      "outputs": [],
      "source": [
        "# This is for checkpointing, if you're doing it over multiple sessions\n",
        "\n",
        "last_epoch_completed = 0\n",
        "start = last_epoch_completed\n",
        "end = config[\"epochs\"]\n",
        "best_lev_dist = float(\"inf\") # if you're restarting from some checkpoint, use what you saw there.\n",
        "epoch_model_path = 'checkpoint_asr5.pth'#TODO set the model path( Optional, you can just store best one. Make sure to make the changes below )\n",
        "best_model_path = 'best_model_asr5.pth'#TODO set best model path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JR43E28rM9Ak",
        "outputId": "f2a2efb4-4150-4c5d-9062-2ecff4ba6e9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 41/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 3.7929\t Learning Rate 0.0020000\n",
            "\tVal Dist 75.1437%\t Val Loss 3.3368\n",
            "Saved epoch model\n",
            "Saved best model\n",
            "\n",
            "Epoch: 42/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 3.0559\t Learning Rate 0.0020000\n",
            "\tVal Dist 52.8509%\t Val Loss 2.3385\n",
            "Saved epoch model\n",
            "Saved best model\n",
            "\n",
            "Epoch: 43/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 2.1301\t Learning Rate 0.0020000\n",
            "\tVal Dist 36.5297%\t Val Loss 1.5397\n",
            "Saved epoch model\n",
            "Saved best model\n",
            "\n",
            "Epoch: 44/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 1.6399\t Learning Rate 0.0020000\n",
            "\tVal Dist 28.1725%\t Val Loss 1.2108\n",
            "Saved epoch model\n",
            "Saved best model\n",
            "\n",
            "Epoch: 45/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 1.4268\t Learning Rate 0.0020000\n",
            "\tVal Dist 21.8642%\t Val Loss 0.9479\n",
            "Saved epoch model\n",
            "Saved best model\n",
            "\n",
            "Epoch: 46/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 1.2087\t Learning Rate 0.0020000\n",
            "\tVal Dist 19.1480%\t Val Loss 0.8361\n",
            "Saved epoch model\n",
            "Saved best model\n",
            "\n",
            "Epoch: 47/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 1.1410\t Learning Rate 0.0020000\n",
            "\tVal Dist 16.6609%\t Val Loss 0.7216\n",
            "Saved epoch model\n",
            "Saved best model\n",
            "\n",
            "Epoch: 48/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 1.0523\t Learning Rate 0.0020000\n",
            "\tVal Dist 15.0795%\t Val Loss 0.6575\n",
            "Saved epoch model\n",
            "Saved best model\n",
            "\n",
            "Epoch: 49/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.9876\t Learning Rate 0.0020000\n",
            "\tVal Dist 14.0931%\t Val Loss 0.6204\n",
            "Saved epoch model\n",
            "Saved best model\n",
            "\n",
            "Epoch: 50/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.9547\t Learning Rate 0.0020000\n",
            "\tVal Dist 14.4579%\t Val Loss 0.6286\n",
            "Saved epoch model\n",
            "\n",
            "Epoch: 51/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.9148\t Learning Rate 0.0020000\n",
            "\tVal Dist 12.4932%\t Val Loss 0.5545\n",
            "Saved epoch model\n",
            "Saved best model\n",
            "\n",
            "Epoch: 52/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.8306\t Learning Rate 0.0020000\n",
            "\tVal Dist 11.0994%\t Val Loss 0.4926\n",
            "Saved epoch model\n",
            "Saved best model\n",
            "\n",
            "Epoch: 53/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.8480\t Learning Rate 0.0020000\n",
            "\tVal Dist 12.0389%\t Val Loss 0.5348\n",
            "Saved epoch model\n",
            "\n",
            "Epoch: 54/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.7818\t Learning Rate 0.0020000\n",
            "\tVal Dist 10.4827%\t Val Loss 0.4677\n",
            "Saved epoch model\n",
            "Saved best model\n",
            "\n",
            "Epoch: 55/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.7858\t Learning Rate 0.0020000\n",
            "\tVal Dist 10.5300%\t Val Loss 0.4718\n",
            "Saved epoch model\n",
            "\n",
            "Epoch: 56/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.7637\t Learning Rate 0.0020000\n",
            "\tVal Dist 9.8272%\t Val Loss 0.4407\n",
            "Saved epoch model\n",
            "Saved best model\n",
            "\n",
            "Epoch: 57/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.7310\t Learning Rate 0.0020000\n",
            "\tVal Dist 9.3754%\t Val Loss 0.4202\n",
            "Saved epoch model\n",
            "Saved best model\n",
            "\n",
            "Epoch: 58/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.7091\t Learning Rate 0.0020000\n",
            "\tVal Dist 9.3445%\t Val Loss 0.4262\n",
            "Saved epoch model\n",
            "Saved best model\n",
            "\n",
            "Epoch: 59/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.7386\t Learning Rate 0.0020000\n",
            "\tVal Dist 9.1360%\t Val Loss 0.4099\n",
            "Saved epoch model\n",
            "Saved best model\n",
            "\n",
            "Epoch: 60/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.6578\t Learning Rate 0.0020000\n",
            "\tVal Dist 8.6305%\t Val Loss 0.3936\n",
            "Saved epoch model\n",
            "Saved best model\n",
            "\n",
            "Epoch: 61/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.6909\t Learning Rate 0.0020000\n",
            "\tVal Dist 8.9117%\t Val Loss 0.4037\n",
            "Saved epoch model\n",
            "\n",
            "Epoch: 62/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.6695\t Learning Rate 0.0020000\n",
            "\tVal Dist 8.5030%\t Val Loss 0.3869\n",
            "Saved epoch model\n",
            "Saved best model\n",
            "\n",
            "Epoch: 63/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.6567\t Learning Rate 0.0020000\n",
            "\tVal Dist 7.9105%\t Val Loss 0.3621\n",
            "Saved epoch model\n",
            "Saved best model\n",
            "\n",
            "Epoch: 64/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.6300\t Learning Rate 0.0020000\n",
            "\tVal Dist 8.0719%\t Val Loss 0.3731\n",
            "Saved epoch model\n",
            "\n",
            "Epoch: 65/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.5925\t Learning Rate 0.0020000\n",
            "\tVal Dist 7.7437%\t Val Loss 0.3594\n",
            "Saved epoch model\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-1592dd1d39cf>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalid_dist\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mbest_lev_dist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mbest_lev_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_dist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'valid_dist'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saved best model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-51903e297c27>\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, optimizer, scheduler, metric, epoch, path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     torch.save(\n\u001b[0m\u001b[1;32m      3\u001b[0m         {'model_state_dict'         : model.state_dict(),\n\u001b[1;32m      4\u001b[0m          \u001b[0;34m'optimizer_state_dict'\u001b[0m     \u001b[0;34m:\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m          \u001b[0;34m'scheduler_state_dict'\u001b[0m     \u001b[0;34m:\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m             \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_end_of_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "#TODO: Please complete the training loop\n",
        "\n",
        "for epoch in range(40, config['epochs']):\n",
        "\n",
        "    print(\"\\nEpoch: {}/{}\".format(epoch+1, config['epochs']))\n",
        "\n",
        "    curr_lr = float(optimizer.param_groups[0]['lr']) #TODO\n",
        "\n",
        "    train_loss              = train_model(model, train_loader, criterion, optimizer)#TODO\n",
        "    valid_loss, valid_dist  = validate_model(model, val_loader, decoder)#TODO\n",
        "    scheduler.step(valid_dist)\n",
        "\n",
        "    print(\"\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_loss, curr_lr))\n",
        "    print(\"\\tVal Dist {:.04f}%\\t Val Loss {:.04f}\".format(valid_dist, valid_loss))\n",
        "\n",
        "\n",
        "    wandb.log({\n",
        "        'train_loss': train_loss,\n",
        "        'valid_dist': valid_dist,\n",
        "        'valid_loss': valid_loss,\n",
        "        'lr'        : curr_lr\n",
        "    })\n",
        "\n",
        "    save_model(model, optimizer, scheduler, ['valid_dist', valid_dist], epoch, epoch_model_path)\n",
        "    wandb.save(epoch_model_path)\n",
        "    print(\"Saved epoch model\")\n",
        "\n",
        "    if valid_dist <= best_lev_dist:\n",
        "        best_lev_dist = valid_dist\n",
        "        save_model(model, optimizer, scheduler, ['valid_dist', valid_dist], epoch, best_model_path)\n",
        "        wandb.save(best_model_path)\n",
        "        print(\"Saved best model\")\n",
        "      # You may find it interesting to exlplore Wandb Artifcats to version your models\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2H4EEj-sD32"
      },
      "source": [
        "# Generate Predictions and Submit to Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkuxBraWSbSc",
        "outputId": "0c748fa9-b84c-4a71-85ee-6809e212b980"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('./best_model_asr4_extra.pth')['model_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2moYJhTWsOG-",
        "outputId": "dc9f289e-181e-4627-9b2f-03bd74c8eefb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/41 [00:00<?, ?it/s]\u001b[A\n",
            "  2%|▏         | 1/41 [00:00<00:25,  1.57it/s]\u001b[A\n",
            "  5%|▍         | 2/41 [00:01<00:26,  1.47it/s]\u001b[A\n",
            "  7%|▋         | 3/41 [00:02<00:26,  1.41it/s]\u001b[A\n",
            " 10%|▉         | 4/41 [00:02<00:24,  1.52it/s]\u001b[A\n",
            " 12%|█▏        | 5/41 [00:03<00:23,  1.54it/s]\u001b[A\n",
            " 15%|█▍        | 6/41 [00:03<00:20,  1.73it/s]\u001b[A\n",
            " 17%|█▋        | 7/41 [00:04<00:16,  2.03it/s]\u001b[A\n",
            " 20%|█▉        | 8/41 [00:04<00:18,  1.79it/s]\u001b[A\n",
            " 22%|██▏       | 9/41 [00:05<00:19,  1.68it/s]\u001b[A\n",
            " 24%|██▍       | 10/41 [00:06<00:19,  1.59it/s]\u001b[A\n",
            " 27%|██▋       | 11/41 [00:06<00:16,  1.83it/s]\u001b[A\n",
            " 29%|██▉       | 12/41 [00:06<00:15,  1.89it/s]\u001b[A\n",
            " 32%|███▏      | 13/41 [00:07<00:13,  2.07it/s]\u001b[A\n",
            " 34%|███▍      | 14/41 [00:07<00:14,  1.90it/s]\u001b[A\n",
            " 37%|███▋      | 15/41 [00:08<00:14,  1.74it/s]\u001b[A\n",
            " 39%|███▉      | 16/41 [00:09<00:15,  1.64it/s]\u001b[A\n",
            " 41%|████▏     | 17/41 [00:10<00:16,  1.49it/s]\u001b[A\n",
            " 44%|████▍     | 18/41 [00:10<00:15,  1.52it/s]\u001b[A\n",
            " 46%|████▋     | 19/41 [00:11<00:12,  1.82it/s]\u001b[A\n",
            " 49%|████▉     | 20/41 [00:11<00:12,  1.66it/s]\u001b[A\n",
            " 51%|█████     | 21/41 [00:12<00:12,  1.60it/s]\u001b[A\n",
            " 54%|█████▎    | 22/41 [00:13<00:11,  1.59it/s]\u001b[A\n",
            " 56%|█████▌    | 23/41 [00:13<00:11,  1.54it/s]\u001b[A\n",
            " 59%|█████▊    | 24/41 [00:14<00:09,  1.82it/s]\u001b[A\n",
            " 61%|██████    | 25/41 [00:14<00:09,  1.65it/s]\u001b[A\n",
            " 63%|██████▎   | 26/41 [00:15<00:08,  1.71it/s]\u001b[A\n",
            " 66%|██████▌   | 27/41 [00:15<00:07,  1.93it/s]\u001b[A\n",
            " 68%|██████▊   | 28/41 [00:16<00:06,  2.07it/s]\u001b[A\n",
            " 71%|███████   | 29/41 [00:16<00:06,  1.97it/s]\u001b[A\n",
            " 73%|███████▎  | 30/41 [00:17<00:05,  2.13it/s]\u001b[A\n",
            " 76%|███████▌  | 31/41 [00:17<00:04,  2.05it/s]\u001b[A\n",
            " 78%|███████▊  | 32/41 [00:18<00:04,  1.85it/s]\u001b[A\n",
            " 80%|████████  | 33/41 [00:18<00:04,  1.81it/s]\u001b[A\n",
            " 83%|████████▎ | 34/41 [00:19<00:03,  1.82it/s]\u001b[A\n",
            " 85%|████████▌ | 35/41 [00:19<00:03,  1.84it/s]\u001b[A\n",
            " 88%|████████▊ | 36/41 [00:20<00:03,  1.54it/s]\u001b[A\n",
            " 90%|█████████ | 37/41 [00:21<00:02,  1.48it/s]\u001b[A\n",
            " 93%|█████████▎| 38/41 [00:22<00:01,  1.62it/s]\u001b[A\n",
            " 95%|█████████▌| 39/41 [00:22<00:01,  1.70it/s]\u001b[A\n",
            " 98%|█████████▊| 40/41 [00:23<00:00,  1.70it/s]\u001b[A\n",
            "100%|██████████| 41/41 [00:23<00:00,  1.72it/s]\n"
          ]
        }
      ],
      "source": [
        "#TODO: Make predictions\n",
        "\n",
        "# Follow the steps below:\n",
        "# 1. Create a new object for CTCBeamDecoder with larger (why?) number of beams\n",
        "# 2. Get prediction string by decoding the results of the beam decoder\n",
        "\n",
        "TEST_BEAM_WIDTH = 2#TODO\n",
        "\n",
        "test_decoder    = CTCBeamDecoder(LABELS, beam_width=TEST_BEAM_WIDTH, log_probs_input=True)#TODO\n",
        "results = []\n",
        "\n",
        "model.eval()\n",
        "print(\"Testing\")\n",
        "for data in tqdm(test_loader):\n",
        "\n",
        "    x, lx   = data\n",
        "    x       = x.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        h, lh = model(x, lx)\n",
        "        #h = torch.permute(h, (1, 0, 2))\n",
        "\n",
        "    prediction_string= decode_prediction(h, lh, test_decoder)# TODO call decode_prediction\n",
        "    #TODO save the output in results array.\n",
        "    results.extend(prediction_string)\n",
        "\n",
        "    del x, lx, h, lh\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d70dvu_lsMlv"
      },
      "outputs": [],
      "source": [
        "results = [\"\".join(arr) for arr in results]\n",
        "data_dir = f\"{root}/test-clean/random_submission.csv\"\n",
        "df = pd.read_csv(data_dir)\n",
        "df.label = results\n",
        "df.to_csv('submission.csv', index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1sZmEIs4yIz",
        "outputId": "c93db13b-2522-4c4a-85de-1ff24f96a2f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.6.7 / client 1.5.8)\n",
            "100% 208k/208k [00:01<00:00, 168kB/s]\n",
            "Successfully submitted to HW3P2_ASR-S24(slack)"
          ]
        }
      ],
      "source": [
        "!kaggle competitions submit -c hw3p2asr-s24slack -f submission.csv -m \"I made it!\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "R9v5ewZDMpYA",
        "HLad4pChcuvX",
        "tUThsowyQdN7",
        "EV7DMPDoMUz2"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10 (main, Feb 16 2023, 02:49:39) [Clang 14.0.0 (clang-1400.0.29.202)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "bd385fe162c5ca0c84973b7dd5c518456272446b2b64e67c2a69f949ca7a1754"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
